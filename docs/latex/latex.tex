\documentclass[11pt, a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{cite}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow}

\title{\textbf{Efficient Adaptation of Audio Spectrogram Transformers for Environmental Sound Classification}}
\author{
    \textbf{Tel Aviv University} \\
    Advanced Topics in Audio Processing using Deep Learning \\
    \vspace{0.5cm} \\
    Yuval Anavi - 318677622 \\
    Eden Avrahami - 207106444 \\
    Guy Yaffe - 207253980 \\
    Tom Nouri - 209402833
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
In this work, we reproduce and validate the Conformer Adapter module proposed by Cappellazzo et al. (2024) for parameter-efficient transfer learning (PETL) of the Audio Spectrogram Transformer (AST). The Conformer Adapter replaces the standard bottleneck adapter with a convolution-augmented module inspired by the Conformer architecture, inserted in a parallel (Pfeiffer) configuration alongside the frozen AST's feed-forward layers. This design trains only $\sim$0.29\% of the model's parameters while approaching or matching full fine-tuning performance.

We evaluate the method on two audio classification benchmarks: ESC-50~\cite{piczak2015esc} and Google Speech Commands V2 (GSC)~\cite{warden2018speechcommands}. On ESC-50, we achieve an average test accuracy of 85.70\% compared to the paper's reported 88.30\%, with the gap primarily attributed to a reduced batch size (16 vs.\ 32) necessitated by GPU memory constraints. On GSC, we achieve 94.43\% accuracy compared to the paper's reported 96.28\%.

\end{abstract}

\section{Introduction}
The evolution of audio classification has seen a decisive shift toward unified End-to-End frameworks. By replacing complex, multi-stage pipelines with architectures like the Audio Spectrogram Transformer (AST)~\cite{gong2021astaudiospectrogramtransformer}, the field has prioritized architectural simplicity and global optimization. The AST model embodies this by treating audio spectrogram patches as visual tokens, utilizing self-attention to capture long-range dependencies without the need for traditional recurrence or manual alignment.

Despite the high discriminative power of these large-scale models, adapting them to specialized tasks through Full Fine-Tuning (FFT) remains a significant bottleneck. Updating the entire parameter set $\theta$ via FFT can be impractical for embedded applications due to resource constraints and the risk of catastrophic interference. This necessitates Parameter-Efficient Transfer Learning (PETL) methods that can adapt the model to downstream tasks while keeping the backbone frozen. However, standard PETL methods such as LoRA~\cite{hu2022loralowrankadaptationlarge} and linear Bottleneck Adapters~\cite{houlsby2019parameterefficienttransferlearningnlp} can underperform on speech tasks where local spatial correlations are critical, as their purely linear design lacks a spatial inductive bias~\cite{cappellazzo2024parameterefficienttransferlearningaudio}.

In this project, we reproduce the Conformer Adapter~\cite{cappellazzo2024parameterefficienttransferlearningaudio} in a Parallel Pfeiffer configuration. This adapter benefits from the introduction of the depthwise convolution layer inspired by the Conformer model~\cite{gulati2020conformerconvolutionaugmentedtransformer}, which captures local spatial correlations while trimming down the number of parameters. Evaluating this method on the ESC-50~\cite{piczak2015esc} and Google Speech Commands (GSC)~\cite{warden2018speechcommands} benchmarks, we demonstrate that updating only 0.29\% of the parameters can bridge the performance gap with full fine-tuning.

\section{Related Work}

\textbf{Audio Spectrogram Transformer (AST):} 
The AST model~\cite{gong2021astaudiospectrogramtransformer} adapts the Vision Transformer architecture \cite{dosovitskiy2021imageworth16x16words} to the audio domain by processing audio spectrograms as sequences of patches. It achieves state-of-the-art results on various audio and speech classification tasks.

\textbf{Parameter-Efficient Transfer Learning (PETL):} 
To mitigate the costs of full fine-tuning, various PETL methods have been developed to freeze the pre-trained backbone and update only a small set of parameters \cite{cappellazzo2024parameterefficienttransferlearningaudio}. Common approaches include LoRA \cite{hu2022loralowrankadaptationlarge}, which leverages low-rank matrix decompositions; Prompt-Tuning \cite{jia2022visualprompttuning}, which prepends learnable continuous embeddings to the input space either at the first layer (Shallow Prompt-Tuning) or uniformly across each transformer layer (Deep Prompt-Tuning); Bottleneck Adapters \cite{pfeiffer2020adapterfusionnondestructivetaskcomposition, houlsby2019parameterefficienttransferlearningnlp}, which insert light subnetworks into transformer layers that down-project, apply a non-linearity, and up-project the hidden state back to its original dimension; BitFit \cite{zaken2022bitfit}, a minimalist baseline that fine-tunes only the bias terms of the pre-trained backbone; and Prefix-Tuning \cite{li2021prefixtuning}, which inserts learnable continuous prompts into the keys and values of the Multi-Head Self-Attention block at every layer. These methods serve as baselines in the original study, against which we compare our reproduced Conformer Adapter results.

\textbf{The Conformer Model:} 
The Conformer \cite{gulati2020conformerconvolutionaugmentedtransformer} is a leading architecture for speech processing that combines transformers with Convolutional Neural Networks (CNNs). Its core convolution module relies heavily on depthwise convolutions to efficiently capture local spatial correlations. The paper we reproduce leverages this module to replace the simple linear layers of standard bottleneck adapters with depthwise convolutions, creating the Conformer Adapter.

\section{Method}
\subsection{Architecture}

\textbf{Frozen AST Backbone.}
Our architecture builds upon the Audio Spectrogram Transformer (AST)~\cite{gong2021astaudiospectrogramtransformer}, a Vision Transformer~\cite{dosovitskiy2021imageworth16x16words} adapted for audio classification. The input audio waveform is converted to a 128-band log-mel spectrogram, which is split into $N$ patches of size $16 \times 16$. Each patch is linearly projected into a token of dimension $d = 768$, producing a sequence $\mathbf{X}_{\text{in}} \in \mathbb{R}^{N \times d}$. Positional embeddings are prepended and added before passing through 12 transformer layers. Each layer comprises a multi-head self-attention (MHSA) sub-layer and a feed-forward network (FFN) sub-layer, both with pre-layer normalization (LN) and residual connections:
\begin{equation}
\hat{\mathbf{X}} = \mathbf{X}_{\text{in}} + \text{MHSA}\!\big(\text{LN}(\mathbf{X}_{\text{in}})\big), \quad
\mathbf{X}_{\text{out}} = \hat{\mathbf{X}} + \text{FFN}\!\big(\text{LN}(\hat{\mathbf{X}})\big).
\label{eq:transformer}
\end{equation}
We use the AST checkpoint pre-trained on ImageNet-21K and AudioSet and freeze the entire backbone during fine-tuning. Only the injected adapter modules, the layer normalization parameters, and a newly added linear classification head are trained.

\textbf{Conformer Adapter Module.}
Following Cappellazzo et al.~\cite{cappellazzo2024parameterefficienttransferlearningaudio}, we inject lightweight \emph{Conformer Adapter} modules into each of the 12 transformer layers. Inspired by the convolution module of the Conformer~\cite{gulati2020conformerconvolutionaugmentedtransformer}, these adapters apply depthwise convolutions to capture local spatial correlations along the token sequence, a property that standard linear bottleneck adapters lack. Given an input $\mathbf{Z} \in \mathbb{R}^{N \times d}$, the adapter computes:
\begin{equation}
\text{Adapter}(\mathbf{Z}) = \text{PwConv}_{\text{up}}\!\Big(\text{SiLU}\!\big(\text{BN}\!\big(\text{DWConv}_k\!\big(\text{GLU}\!\big(\text{PwConv}_{\text{down}}(\mathbf{Z})\big)\big)\big)\big)\Big),
\label{eq:conformer_adapter}
\end{equation}
where $\text{PwConv}_{\text{down}}$ is a pointwise ($1{\times}1$) convolution projecting $d \!\to\! 2r$ with bottleneck dimension $r = \lfloor d/\text{RR} \rfloor$ and reduction rate $\text{RR}$; GLU halves the channels from $2r$ to $r$; $\text{DWConv}_k$ is a depthwise 1-D convolution with kernel size $k$ and $\text{groups}{=}r$; BN and SiLU denote Batch Normalization and the Swish activation, respectively; and $\text{PwConv}_{\text{up}}$ restores the dimension $r \!\to\! d$. In our experiments we set $\text{RR}=96$ (giving $r=8$) and $k=8$ for ESC-50 and $k=31$ for GSC.

\textbf{Integration (Pfeiffer Parallel).}
We adopt the Pfeiffer configuration~\cite{pfeiffer2020adapterfusionnondestructivetaskcomposition}, inserting one adapter per layer in parallel with the FFN sub-layer. Both the FFN and the adapter receive the same layer-normalized hidden state, and their outputs are summed:
\begin{equation}
\mathbf{X}_{\text{out}} = \hat{\mathbf{X}} + \text{FFN}\!\big(\text{LN}(\hat{\mathbf{X}})\big) + \text{Adapter}\!\big(\text{LN}(\hat{\mathbf{X}})\big).
\label{eq:pfeiffer_parallel}
\end{equation}
After the final transformer layer, classification is performed by averaging all output token representations and passing the result through a task-specific linear classification head.

% --- Figure Section ---
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{arch.png}
    \caption{Architecture overview: (Left) The frozen AST backbone with parallel Conformer Adapters. (Right) Detailed schematic of the Conformer Adapter module featuring depthwise convolutions.}
    \label{fig:architecture}
\end{figure}

\subsection{Evaluation Metrics}
We report classification accuracy as the primary metric. For ESC-50, we follow the standard 5-fold cross-validation protocol~\cite{piczak2015esc}: in each iteration, three folds are used for training, one for validation, and the remaining fold for testing. The final accuracy is the mean $\pm$ standard deviation across the five test folds. For GSC, which provides predefined train/validation/test splits, we report single-run test accuracy.

\subsection{Experimental Setup}
We use the pre-trained AST checkpoint (\texttt{MIT/ast-finetuned-audioset-10-10-0.4593}) with $\sim$85.5M frozen parameters ($d{=}768$, 12 layers). The Conformer Adapter is inserted in Pfeiffer parallel configuration with $\text{RR}{=}96$ ($r{=}8$), $k{=}8$ for ESC-50 and $k{=}31$ for GSC, yielding $\sim$271K trainable parameters (0.29\% of the model).

We optimize with AdamW~\cite{loshchilov2019decoupledweightdecayregularization} ($\beta_1{=}0.9$, $\beta_2{=}0.98$, $\epsilon{=}10^{-6}$, lr$\,{=}\,0.005$, weight decay$\,{=}\,0.1$) and cosine annealing, training for 50 epochs per fold with cross-entropy loss. Input audio is resampled to 16\,kHz, converted to a 128-band log-mel spectrogram, and split into $16{\times}16$ patches ($N{=}500$ tokens) with SpecAugment-style masking.

Training was conducted on a university SLURM cluster (NVIDIA TITAN Xp, 12\,GB VRAM) using the authors' exact software stack (Python 3.10, PyTorch 1.13.1, \texttt{transformers} 4.28.1). We reduced the batch size from 32 to 16 due to GPU memory constraints and parallelized the 5 folds via SLURM job arrays ($\sim$1 hour per fold).

\subsection{Implementation Challenges}
Reproducing results from the authors' repository\footnote{\url{https://github.com/umbertocappellazzo/PETL\_AST}} required overcoming several challenges. \textbf{(1) Google Colab incompatibility:} The authors' dependencies (\texttt{torch==1.13.1}, \texttt{transformers==4.28.1}) are incompatible with Colab's CUDA 12.x / Python 3.12 environment. Bridging the gap with newer libraries led to persistent \texttt{NaN} training losses caused by the \texttt{ASTSdpaAttention} class in modern \texttt{transformers}, which could not be resolved. We migrated to a SLURM cluster. \textbf{(2) Repository bugs:} Missing \texttt{epochs\_ESC} key in the YAML config, \texttt{type=bool} argparse misuse, and malformed checkpoint paths. \textbf{(3) Cluster constraints:} A 4\,GB home directory quota required using alternative storage; \texttt{librosa==0.9.2} needed a pinned \texttt{setuptools==69.5.1} (newer versions removed \texttt{pkg\_resources}); the 12\,GB GPU VRAM required halving the batch size; and the 8-hour job limit required parallelizing folds via SLURM job arrays.

\section{Results and Discussion}
We evaluated the Conformer Adapter (Pfeiffer parallel) on both benchmarks. On ESC-50 (5-fold CV, 50 epochs per fold), our reproduction achieved an average test accuracy of \textbf{85.70\% $\pm$ 1.16\%}, compared to the paper's reported 88.30\%~\cite{cappellazzo2024parameterefficienttransferlearningaudio}, with a best validation accuracy of 87.75\% $\pm$ 1.41\%. On GSC, we achieved \textbf{94.43\%} compared to the paper's 96.28\%.

We attribute the ESC-50 gap (2.6\%) primarily to the reduced batch size (16 vs.\ the authors' 32), necessitated by the 12\,GB VRAM of the TITAN Xp GPUs on our cluster. Smaller batches degrade the Batch Normalization statistics within the adapter's depthwise convolution layer. Despite this, our best validation accuracy (87.75\%) closely approaches the paper's result, suggesting comparable learning capacity.

\subsection{Convergence Analysis}
Figures~\ref{fig:loss} and~\ref{fig:accuracy} show convergence for GSC and ESC-50. Training loss drops sharply in the first 10 epochs and plateaus near zero by epoch 20. Validation accuracy stabilizes around epoch 15--20 with no signs of overfitting and low fold-to-fold variance.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{convergence_fold0.png}
    \caption{GSC: Training loss and accuracy of the model during training}
    \label{fig:loss}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{second_photo.png}
    \caption{ESC: Training loss and accuracy of the model during training}
    \label{fig:accuracy}
\end{figure}

\section{Future Work}
We propose two refinements to the Conformer Adapter:

\begin{itemize}
\item \textbf{Layer-Selective Adapter Injection:} Rather than injecting adapters uniformly across all 12 layers, we propose analyzing per-layer representations to identify which depths benefit most from the spatial inductive bias. Limiting injection to high-impact layers would reduce parameter count while potentially improving performance.

\item \textbf{Signal-Aware Dynamic Gating:} The current Pfeiffer parallel design uses static summation of FFN and adapter outputs. Replacing this with a learnable gate that dynamically weights each branch based on the input signal would allow the model to prioritize local convolutional features for complex sounds while relying on the frozen backbone for simpler patterns.
\end{itemize}

\section{Limitations and Broader Impact}
\textbf{Broader Impact.} By requiring only 0.29\% of parameters to be updated, PETL methods democratize access to high-performance audio models for users with limited compute, without requiring expensive hardware.

\textbf{Limitations.} Optimal hyperparameters do not generalize across tasks: the depthwise convolution kernel size varies between $k{=}8$ (ESC-50) and $k{=}31$ (GSC), and the best adapter configuration (Pfeiffer vs.\ Houlsby) is also task-dependent. This necessitates per-task tuning, complicating the development of a truly general-purpose audio encoder.

\textbf{Risks.} Lowering the computational barrier to fine-tuning simultaneously increases accessibility for malicious actors, potentially enabling unauthorized acoustic surveillance or other harmful audio classification applications.

\bibliographystyle{IEEEtran} % Or 'plain', 'unsrt', or 'alpha'
\bibliography{references}

\end{document}
